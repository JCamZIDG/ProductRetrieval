# Retrieval & RAG Service â€” Product Search Evaluation

Proyecto entregable: evaluaciÃ³n e implementaciÃ³n de un sistema de recuperaciÃ³n de productos (retrieval) con mejora por reranking y un microservicio RAG (opcional LLM local).  
Contiene: (1) notebook exploratorio donde se desarrollÃ³/ evaluÃ³ la soluciÃ³n; (2) microservicio FastAPI que expone bÃºsqueda, reranking y RAG.

---

## Resumen

Este proyecto implementa y evalÃºa pipelines de bÃºsqueda de productos sobre el dataset WANDS:

- **Baseline lexical**: TF-IDF + lematizaciÃ³n (spaCy).
- **Baseline semÃ¡ntico**: embeddings (Sentence-Transformers `all-MiniLM-L6-v2`) + FAISS.
- **Reranking**: Cross-Encoder (`cross-encoder/ms-marco-MiniLM-L-6-v2`).
- **RAG (opcional)**: IntegraciÃ³n con LLM local usando `gpt4all` para generar explicaciones / justificar la selecciÃ³n (no obligatorio para que el servicio funcione; si el LLM no estÃ¡ disponible, se usa fallback determinÃ­stico).
- MÃ©tricas de evaluaciÃ³n incluidas: **MAP@10**, **Weighted MAP@10**, **NDCG@10**, **Precision@10**, **Recall@10**, plus bootstrap paired test.

Objetivo de la entrega: mejorar MAP@10 sobre el baseline y refactorizar el pipeline como microservicio listo para pruebas.

---

## Estructura del repositorio

```
RetrievalBestProductsMatch/
â”œâ”€ app/
â”‚ â”œâ”€ init.py
â”‚ â”œâ”€ config.py # carga de env vars (pydantic)
â”‚ â”œâ”€ logger.py # configuraciÃ³n logging
â”‚ â”œâ”€ models/
â”‚ â”‚ â”œâ”€ retriever.py # clase Retriever (FAISS + embeddings)
â”‚ â”‚ â”œâ”€ reranker.py # clase Reranker (CrossEncoder)
â”‚ â”‚ â””â”€ rag.py # clase RAGService (build context + call LLM/fallback)
â”‚ â”œâ”€ clients/
â”‚ â”‚ â”œâ”€ gpt4all_client.py # Adaptador para gpt4all
â”‚ â”‚ â””â”€ llm_base.py # Interfaz LLMClientProtocol
â”‚ â”œâ”€ utils/
â”‚ â”‚ â””â”€ llm_loader.py # Loader que instancia GPT4All con descarga opcional
â”‚ â”œâ”€ data/ # No incluir datos privados en el repo
â”‚ â”‚ â”œâ”€ faiss.index # Ã­ndice FAISS (generado por el notebook)
â”‚ â”‚ â”œâ”€ label.csv # (opcional local para pruebas)
â”‚ â”‚ â”œâ”€ query.csv
â”‚ â”‚ â””â”€ product.csv
â”‚ â”œâ”€ schemas.py # pydantic request/response models
â”‚ â””â”€ utils.py # helpers (metrics, parsing)
â”œâ”€ tests/ # tests unitarios (opcional)
â”œâ”€ main.py # FastAPI app + endpoints
```

notebook.ipynb # notebook con todo el desarrollo/evaluaciÃ³n
requirements.txt
README.md

## Requisitos

El proyecto fue desarrollado con las dependencias listadas en `requirements.txt`. Para instalar:

```bash
python -m venv .venv
source .venv/bin/activate        # Linux / macOS
.venv\Scripts\activate           # Windows

pip install -r requirements.txt
```

## Notebook: reproducir experimentos

1. Abre `AIEngineer_retrieval_assignment_JuanCamarena.ipynb` en Jupyter / Colab (preferible ejecutar local con GPU si la tienes).

2. AsegÃºrate de colocar `product.csv`, `query.csv`, `label.csv` en `app/data/` o actualizar las rutas.

3. Ejecuta todas las celdas (restart & run all) para:
   - generar embeddings,
   - construir y guardar `data/faiss.index`,
   - evaluar baselines (TF-IDF) y pipelines semÃ¡ntico / rerank,
   - ejecutar grid-search para `partial_w` y bootstrap paired test.

El notebook incluye celdas Markdown con justificaciÃ³n, resultados y visualizaciones (grÃ¡ficas comparativas, top queries, etc).

**Importante:** el notebook genera `data/faiss.index`. Si vas a levantar el microservicio, asegÃºrate de que `FAISS_INDEX_PATH` apunte a ese archivo (o genera el Ã­ndice desde el notebook primero).

---

## Microservicio (FastAPI)

### Ejecutar localmente
```bash
uvicorn app.main:app --reload 
```

## ğŸš€ EjecuciÃ³n del Microservicio (FastAPI)

Por defecto el servidor arranca en:  
ğŸ‘‰ **http://127.0.0.1:8000**


## ğŸ§  Configurar y usar LLM local (gpt4all)

Si deseas usar RAG con un LLM local:

- Coloca el archivo `.gguf` en una ruta accesible, **o**  
- Deja `LLM_ALLOW_DOWNLOAD=true` para que **gpt4all** lo descargue al cachÃ© del usuario en la primera ejecuciÃ³n.

âš ï¸ **Advertencia:**  
Los modelos grandes (7B, 8B) ocupan varios GB y la descarga puede tardar o fallar en redes lentas.  
TambiÃ©n requieren bastante RAM/GPU.

Si **gpt4all** no puede cargar el modelo, el servicio seguirÃ¡ funcionando **sin LLM**.

---

## ğŸ“Š Resultados clave (experimentales)

Comparativa entre **Semantic (FAISS)** y **Rerank (FAISS â†’ Cross-Encoder)**:

### ğŸ”¹ Semantic (FAISS top-10) â€” promedios
| MÃ©trica | Valor |
|----------|--------|
| MAP@10 (exact only) | 0.3353 |
| Weighted MAP@10 | 0.5342 |
| NDCG@10 (graded) | 0.8713 |
| Precision@10 | 0.3223 |
| Recall@10 | 0.1729 |

---

### ğŸ”¹ Rerank (FAISS â†’ CrossEncoder â†’ top-10) â€” promedios
| MÃ©trica | Valor |
|----------|--------|
| MAP@10 (exact only) | 0.4411 |
| Weighted MAP@10 | 0.5971 |
| NDCG@10 (graded) | 0.8942 |
| Precision@10 | 0.3769 |
| Recall@10 | 0.2213 |

---

**Bootstrap paired (Weighted MAP):**  
- mean difference = **0.062882**  
- 95% CI = **(0.048660, 0.077632)** â†’ mejora estadÃ­sticamente significativa âœ…

---

## âš™ï¸ Grid Search `partial_w`

Valores probados: `[0.2, 0.4, 0.5, 0.7, 0.9]`  
â†’ Se usÃ³ `partial_w = 0.5` por defecto como compromiso razonable.

---

## ğŸ’¡ Â¿Por quÃ© incluir *Weighted MAP* (Partial matches)?

El dataset etiqueta coincidencias como **Exact** y **Partial**.  
Tratar `Partial` como irrelevante penaliza demasiado el sistema, cuando el resultado puede ser Ãºtil aunque no sea idÃ©ntico.

**Weighted MAP** permite reconocer grados de relevancia:

- Exact = 1.0  
- Partial = `partial_w`

Esto da una evaluaciÃ³n mÃ¡s realista.

## Proximo Pasos y Desarrollo del Proyecto

Dado el tiempo limitado, el proceso del desarrollo del microservicio fue en su mayorÃ­a un proceso de vibe coding en el que se llevÃ³ lo creado lo del notebook al microservicio lo mÃ¡s rÃ¡pido posible sin perder los resultados obtenidos. De lo cual, desde mi punto de vista personal se logrÃ³ el objetivo.

Al microservicio de todas maneras se le podrÃ­an hacer muchas mejoras de todas maneras, los logs podrÃ­an ser aÃºn mejores; asÃ­ como tambiÃ©n el manejo de errores se podrÃ­a ver en una mayor magnitud implementando tests o mocking para ver realmente que tan efectivo es el manejo de errores del producto final.

En cuanto al modelo, dado que ya se logrÃ³ probar la efectividad con un modelo con mas contexto, tal vez el siguiente camino podrÃ­a ser usar un modelo de NLP mÃ¡s pesado o mÃ¡s enfocado al tipo de requerimiento tenemos, y no uno tan "general"; dado que este producto estaba enfocado precisamente en la comparaciÃ³n de productos a la venta. Tal vez, mejorar el preprocesamiento podrÃ­a llevarnos a aÃºn mejores resultados tambiÃ©n.

Muchas gracias por la evaluaciÃ³n, fue muy divertida de realizar.

Juan Camarena

PD: TerminÃ© modificando las versiones de algunas librerÃ­as (pandas) porque demorarme en temas de compatibilidad por la limitaciÃ³n del tiempo podrÃ­a haberme quitado tiempo valioso que invertir en ideas.